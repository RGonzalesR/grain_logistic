# ğŸ“¦ Grain Logistic - Pipeline de Dados

Projeto de engenharia de dados desenvolvido para processar e transformar os dados logÃ­sticos da Grain Logistic. Utiliza o ecossistema Spark/Delta Lake em Databricks, aplicando boas prÃ¡ticas de ingestÃ£o, validaÃ§Ã£o, modelagem dimensional e geraÃ§Ã£o de features histÃ³ricas.

---

## â„¹ï¸ Disclaimer Inicial

Este projeto foi desenvolvido e testado na **Databricks Community Edition**, o que impÃµe algumas limitaÃ§Ãµes importantes. Alguns recursos que **nÃ£o puderam ser utilizados ou foram simulados** incluem:

- âŒ **Unity Catalog**  
  Recursos de governanÃ§a, ACLs (Access Control Lists) e centralizaÃ§Ã£o de permissÃµes nÃ£o estÃ£o disponÃ­veis. Toda a gestÃ£o de permissÃµes e catÃ¡logos foi omitida.

- âŒ **VACUUM com retenÃ§Ã£o mÃ­nima personalizada**  
  A execuÃ§Ã£o de `VACUUM` com retenÃ§Ã£o inferior a 7 dias estÃ¡ bloqueada, dificultando a remoÃ§Ã£o imediata de arquivos obsoletos apÃ³s sobrescritas.

- âŒ **Monitoramento de Jobs (Job UI)**  
  A criaÃ§Ã£o de jobs agendados e monitoramento de execuÃ§Ãµes em painel dedicado nÃ£o estÃ¡ disponÃ­vel. A orquestraÃ§Ã£o foi realizada por notebooks encadeados via `%run`.

- âŒ **Delta Live Tables (DLT)**  
  A criaÃ§Ã£o de pipelines declarativos com validaÃ§Ã£o contÃ­nua e versionamento automÃ¡tico de tabelas nÃ£o estÃ¡ disponÃ­vel.

- âŒ **Auto Loader (cloudFiles)**  
  A ingestÃ£o incremental automÃ¡tica de arquivos (sem necessidade de controle de checkpoint manual) foi substituÃ­da por `spark.read.csv`.

- âŒ **Dashboards interativos e widgets HTML**  
  A criaÃ§Ã£o de dashboards e visualizaÃ§Ãµes interativas diretamente nos notebooks foi evitada em razÃ£o das restriÃ§Ãµes de UI da Community Edition.

Apesar dessas limitaÃ§Ãµes, o projeto foi estruturado de forma que possa ser facilmente adaptado para ambientes Databricks Premium ou Enterprise.


---

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ 01\_ingestao\_bronze.py
â”œâ”€â”€ silver/
â”‚   â””â”€â”€ 02\_transformacoes\_silver.py
â”œâ”€â”€ gold/
â”‚   â”œâ”€â”€ 03\_dim\_uf.py
â”‚   â”œâ”€â”€ 04\_dim\_calendario.py
â”‚   â”œâ”€â”€ 05\_dim\_importancia.py
â”‚   â””â”€â”€ 06\_fato\_venda.py
â”œâ”€â”€ feature\_store/
â”‚   â””â”€â”€ 07\_fs\_historico\_envios\_corredor.py
â”œâ”€â”€ qualidade/
â”‚   â”œâ”€â”€ define\_schema.py
â”‚   â”œâ”€â”€ valida\_integridade.py
â”‚   â””â”€â”€ valida\_presenca\_coluna.py
â”œâ”€â”€ testes/
â”‚   â”œâ”€â”€ 00-run\_all.py
â”‚   â”œâ”€â”€ 01-teste\_schemas.py
â”‚   â”œâ”€â”€ 02-teste\_integracao\_fato.py
â”‚   â””â”€â”€ 03-teste\_nulos\_duplicados.py
â”œâ”€â”€ 00-orquestrador.py
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## ğŸš€ Pipeline por Camadas

### ğŸ”¹ Bronze

- **Script:** `01_ingestao_bronze.py`
- Leitura do arquivo `grain_logistic_shipping.csv`
- Tipagem forÃ§ada para `StringType` (evitando perda de informaÃ§Ã£o)
- AdiÃ§Ã£o de metadados (usuÃ¡rio, modo de ingestÃ£o, timestamp)
- Escrita em Delta Lake particionada por `data_ingestao`
- OtimizaÃ§Ã£o com `OPTIMIZE` e `ZORDER BY`

---

### ğŸ”¸ Silver

- **Script:** `02_transformacoes_silver.py`
- PadronizaÃ§Ã£o de colunas
- TransformaÃ§Ã£o em 3 tabelas:
  - `envios`: informaÃ§Ãµes logÃ­sticas e temporais
  - `produto`: preÃ§o, peso, desconto
  - `cliente`: ligaÃ§Ãµes, gÃªnero, avaliaÃ§Ãµes
- AplicaÃ§Ã£o de validaÃ§Ãµes e definiÃ§Ã£o de schema

---

### ğŸŸ¡ Gold

- **DimensÃµes**
  - `dim_uf`: unidade federativa e sigla
  - `dim_calendario`: data completa, dia da semana, mÃªs, ano
  - `dim_importancia`: grau de urgÃªncia (low, medium, high)
- **Fato**
  - `fato_venda`: mÃ©tricas da venda (receita, tempo de entrega, peso total)
  - Particionada por `id_uf` e otimizada com `ZORDER`

---

### ğŸ§  Feature Store

- **Script:** `07_fs_historico_envios_corredor.py`
- GeraÃ§Ã£o de histÃ³rico acumulado diÃ¡rio por corredor logÃ­stico
- MÃ©tricas: volume de vendas, tempo mÃ©dio de entrega, avaliaÃ§Ãµes, gÃªnero
- Particionamento por `ds_corredor_de_armazenagem`
- OtimizaÃ§Ã£o com `ZORDER`

---

## âœ… ValidaÃ§Ãµes

- **PresenÃ§a de colunas esperadas**: `valida_presenca_coluna.py`
- **Integridade de dados**: `valida_integridade.py`
- **DefiniÃ§Ã£o padronizada de schema**: `define_schema.py`

---

## ğŸ§ª Testes

Localizados na pasta `testes/`:

- `01-teste_schemas.py`: ValidaÃ§Ã£o de schemas das tabelas
- `02-teste_integracao_fato.py`: VerificaÃ§Ã£o de integridade na `fato_venda`
- `03-teste_nulos_duplicados.py`: Checagem de valores nulos e duplicados
- `00-run_all.py`: ExecuÃ§Ã£o centralizada dos testes

---

## ğŸ§­ OrquestraÃ§Ã£o

- **Script:** `00-orquestrador.py`
- Executa todos os notebooks da pipeline com ordem de dependÃªncia entre camadas

---

## ğŸ“„ LicenÃ§a

DistribuÃ­do sob a licenÃ§a MIT. Veja `LICENSE` para mais detalhes.

---

## âœï¸ Autor

Renan Gonzales  
Engenheiro de Dados
